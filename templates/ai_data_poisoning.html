{% extends "base.html" %}

{% block title %}Data Poisoning Attack - VulnShop{% endblock %}

{% block content %}
<div class="row">
    <div class="col-12">
        <div class="card mb-4">
            <div class="card-header bg-warning text-dark">
                <h3 class="mb-0"><i class="fas fa-skull-crossbones"></i> Data Poisoning Attack</h3>
            </div>
            <div class="card-body">
                <div class="alert alert-warning">
                    <h5><i class="fas fa-exclamation-triangle"></i> Vulnerability: Unvalidated Training Data</h5>
                    <p class="mb-0">
                        This spam classifier accepts user feedback to retrain itself. 
                        Inject poisoned data to manipulate the model's behavior!
                    </p>
                </div>

                <div class="row">
                    <div class="col-md-6">
                        <h5><i class="fas fa-info-circle"></i> About Data Poisoning</h5>
                        <p>
                            Data poisoning attacks corrupt machine learning training data to manipulate model behavior.
                            Attackers inject malicious samples that cause the model to learn incorrect patterns.
                        </p>
                        
                        <h6>Attack Types:</h6>
                        <ul>
                            <li><strong>Label Flipping:</strong> Change correct labels to wrong ones</li>
                            <li><strong>Backdoor Injection:</strong> Plant triggers for specific behaviors</li>
                            <li><strong>Feature Manipulation:</strong> Modify input features strategically</li>
                            <li><strong>Availability Attacks:</strong> Degrade overall model performance</li>
                        </ul>

                        <h6>Real-World Cases:</h6>
                        <ul>
                            <li>Microsoft Tay chatbot (2016) - corrupted via Twitter</li>
                            <li>Facial recognition poisoning in law enforcement</li>
                            <li>Spam filter bypass through feedback manipulation</li>
                        </ul>

                        <div class="alert alert-info mt-3">
                            <strong>Challenge:</strong>
                            <ul class="mb-0">
                                <li>The spam classifier has 85% accuracy initially</li>
                                <li>Submit poisoned training samples to reduce its accuracy</li>
                                <li>Goal: Drop accuracy below 60% by label flipping</li>
                            </ul>
                        </div>
                    </div>

                    <div class="col-md-6">
                        <h5><i class="fas fa-robot"></i> Spam Classifier</h5>
                        
                        <!-- Current Model Stats -->
                        <div class="card mb-3">
                            <div class="card-body">
                                <h6>Model Statistics</h6>
                                <div class="progress mb-2" style="height: 25px;">
                                    <div id="accuracyBar" class="progress-bar bg-success" role="progressbar" 
                                         style="width: 85%;" aria-valuenow="85" aria-valuemin="0" aria-valuemax="100">
                                        85% Accuracy
                                    </div>
                                </div>
                                <small class="text-muted">Training samples: <span id="sampleCount">1000</span></small>
                            </div>
                        </div>

                        <!-- Test Classification -->
                        <div class="card mb-3">
                            <div class="card-header">
                                <strong>Test Classification</strong>
                            </div>
                            <div class="card-body">
                                <form id="testForm">
                                    <div class="mb-2">
                                        <input type="text" class="form-control" id="testMessage" 
                                               placeholder="Enter message to classify..." required>
                                    </div>
                                    <button type="submit" class="btn btn-primary btn-sm w-100">
                                        <i class="fas fa-search"></i> Classify
                                    </button>
                                </form>
                                <div id="testResult" class="mt-2"></div>
                            </div>
                        </div>

                        <!-- Submit Training Data -->
                        <div class="card">
                            <div class="card-header bg-warning">
                                <strong>Submit Training Feedback</strong>
                            </div>
                            <div class="card-body">
                                <form id="poisonForm">
                                    <div class="mb-2">
                                        <label class="form-label">Message:</label>
                                        <input type="text" class="form-control" id="trainMessage" 
                                               placeholder="Message content" required>
                                    </div>
                                    <div class="mb-2">
                                        <label class="form-label">Label:</label>
                                        <select class="form-select" id="trainLabel" required>
                                            <option value="spam">Spam</option>
                                            <option value="ham">Not Spam (Ham)</option>
                                        </select>
                                    </div>
                                    <button type="submit" class="btn btn-warning w-100">
                                        <i class="fas fa-database"></i> Submit Training Sample
                                    </button>
                                </form>
                                <div id="poisonResult" class="mt-2"></div>

                                <div class="mt-3">
                                    <small class="text-muted">
                                        <strong>Hint:</strong> Submit legitimate spam messages labeled as "ham" to poison the dataset
                                    </small>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
let currentAccuracy = 85;
let sampleCount = 1000;
let poisonedSamples = 0;

// Test classification
document.getElementById('testForm').addEventListener('submit', async function(e) {
    e.preventDefault();
    
    const message = document.getElementById('testMessage').value;
    const resultDiv = document.getElementById('testResult');
    
    try {
        const response = await fetch('/ai-data-poisoning', {
            method: 'POST',
            headers: {'Content-Type': 'application/x-www-form-urlencoded'},
            body: `action=classify&message=${encodeURIComponent(message)}`
        });
        
        const data = await response.json();
        
        const badgeClass = data.prediction === 'spam' ? 'bg-danger' : 'bg-success';
        resultDiv.innerHTML = `
            <div class="alert alert-secondary mb-0">
                <strong>Prediction:</strong> <span class="badge ${badgeClass}">${data.prediction.toUpperCase()}</span><br>
                <small>Confidence: ${data.confidence}%</small>
            </div>
        `;
    } catch (error) {
        console.error('Error:', error);
    }
});

// Submit training data (poisoning)
document.getElementById('poisonForm').addEventListener('submit', async function(e) {
    e.preventDefault();
    
    const message = document.getElementById('trainMessage').value;
    const label = document.getElementById('trainLabel').value;
    const resultDiv = document.getElementById('poisonResult');
    
    try {
        const response = await fetch('/ai-data-poisoning', {
            method: 'POST',
            headers: {'Content-Type': 'application/x-www-form-urlencoded'},
            body: `action=train&message=${encodeURIComponent(message)}&label=${label}`
        });
        
        const data = await response.json();
        
        // Update stats
        currentAccuracy = data.new_accuracy;
        sampleCount = data.sample_count;
        
        const accuracyBar = document.getElementById('accuracyBar');
        accuracyBar.style.width = currentAccuracy + '%';
        accuracyBar.textContent = currentAccuracy + '% Accuracy';
        
        if (currentAccuracy < 70) {
            accuracyBar.classList.remove('bg-success');
            accuracyBar.classList.add('bg-warning');
        }
        if (currentAccuracy < 60) {
            accuracyBar.classList.remove('bg-warning');
            accuracyBar.classList.add('bg-danger');
        }
        
        document.getElementById('sampleCount').textContent = sampleCount;
        
        // Show result
        if (data.poisoned) {
            resultDiv.innerHTML = `
                <div class="alert alert-danger mb-0 mt-2">
                    <i class="fas fa-check-circle"></i> <strong>Poisoning successful!</strong><br>
                    Model accuracy dropped to ${currentAccuracy}%
                </div>
            `;
        } else {
            resultDiv.innerHTML = `
                <div class="alert alert-success mb-0 mt-2">
                    Training sample added successfully
                </div>
            `;
        }
    } catch (error) {
        console.error('Error:', error);
    }
    
    // Clear form
    document.getElementById('trainMessage').value = '';
});
</script>
{% endblock %}
